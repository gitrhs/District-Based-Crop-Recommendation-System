{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (11002, 6)\n",
      "\n",
      "First few rows:\n",
      "         date  state    district   crop_type  crop_species  production\n",
      "0  2017-01-01  Johor  Batu Pahat  cash_crops       cassava       920.5\n",
      "1  2017-01-01  Johor  Batu Pahat  cash_crops    groundnuts         0.0\n",
      "2  2017-01-01  Johor  Batu Pahat  cash_crops    sweet_corn         0.0\n",
      "3  2017-01-01  Johor  Batu Pahat  cash_crops  sweet_potato       350.0\n",
      "4  2017-01-01  Johor  Batu Pahat  cash_crops           yam       395.4\n",
      "\n",
      "Column info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11002 entries, 0 to 11001\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   date          11002 non-null  object \n",
      " 1   state         11002 non-null  object \n",
      " 2   district      11002 non-null  object \n",
      " 3   crop_type     11002 non-null  object \n",
      " 4   crop_species  11002 non-null  object \n",
      " 5   production    11002 non-null  float64\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 515.8+ KB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      "date            0\n",
      "state           0\n",
      "district        0\n",
      "crop_type       0\n",
      "crop_species    0\n",
      "production      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataset/Crops_District_Production.csv')\n",
    "# basic Information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn info:\")\n",
    "print(df.info())\n",
    "# Check for any missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed dataset shape: (903, 8)\n",
      "\n",
      "First 10 rows:\n",
      "   state     district         crop_type           crop_rank_1  \\\n",
      "0  Johor   Batu Pahat        cash_crops               cassava   \n",
      "1  Johor   Batu Pahat             fruit             pineapple   \n",
      "2  Johor   Batu Pahat             herbs  fragrant_lemon_grass   \n",
      "3  Johor   Batu Pahat  industrial_crops               coconut   \n",
      "4  Johor   Batu Pahat            spices       calamondin_lime   \n",
      "5  Johor   Batu Pahat         vegetable             long_bean   \n",
      "6  Johor  Johor Bahru        cash_crops               cassava   \n",
      "7  Johor  Johor Bahru             fruit                banana   \n",
      "8  Johor  Johor Bahru             herbs  fragrant_lemon_grass   \n",
      "9  Johor  Johor Bahru  industrial_crops               coconut   \n",
      "\n",
      "         crop_rank_2       crop_rank_3       crop_rank_4   crop_rank_5  \n",
      "0                yam      sweet_potato        groundnuts    sweet_corn  \n",
      "1             papaya            durian            banana         guava  \n",
      "2              cekur      lemon_myrtle         aloe_vera         basil  \n",
      "3             coffee           roselle          mushroom     areca_nut  \n",
      "4        lemon_grass  greater_galangal  pink_cone_ginger          lime  \n",
      "5       ladys_finger           brinjal          cucumber  bitter_gourd  \n",
      "6  yellow_sugar_cane      sweet_potato               yam    sweet_corn  \n",
      "7          pineapple            durian          rambutan        papaya  \n",
      "8          aloe_vera             basil     belalai_gajah    betel_vine  \n",
      "9           mushroom         areca_nut            coffee     honey_bee  \n"
     ]
    }
   ],
   "source": [
    "# Group by state, district, crop_type and find top 5 species by production\n",
    "def transform_to_top5_format(df):\n",
    "    \"\"\"\n",
    "    Transform crop data to show top 5 species per district/crop_type\n",
    "    Modern pandas approach that avoids all warnings\n",
    "    \"\"\"\n",
    "    # Sort by production in descending order\n",
    "    df_sorted = df.sort_values(['state', 'district', 'crop_type', 'production'], \n",
    "                              ascending=[True, True, True, False])\n",
    "    \n",
    "    # Get top 5 for each group and explicitly create a copy\n",
    "    top5_df = df_sorted.groupby(['state', 'district', 'crop_type']).head(5).copy()\n",
    "    \n",
    "    # Create ranking within each group using .loc to avoid warning\n",
    "    top5_df.loc[:, 'rank'] = top5_df.groupby(['state', 'district', 'crop_type']).cumcount() + 1\n",
    "    \n",
    "    # Pivot to get the desired format\n",
    "    result = top5_df.pivot_table(\n",
    "        index=['state', 'district', 'crop_type'],\n",
    "        columns='rank',\n",
    "        values='crop_species',\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns to crop_rank_1, crop_rank_2, etc.\n",
    "    result.columns.name = None  # Remove the 'rank' column name\n",
    "    new_columns = ['state', 'district', 'crop_type'] + [f'crop_rank_{i}' for i in range(1, 6)]\n",
    "    result.columns = new_columns[:len(result.columns)]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Transform the data\n",
    "df = transform_to_top5_format(df)\n",
    "\n",
    "# Display results\n",
    "print(\"Transformed dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load all CSV files...\n",
      "District data and Corps shape: (903, 8)\n",
      "Weather information shape: (155, 14)\n",
      "District elevation and slope shape: (155, 9)\n",
      "District soil type shape: (155, 7)\n"
     ]
    }
   ],
   "source": [
    "# Combine all csv data into single dataset:\n",
    "print(\"Load all CSV files...\")\n",
    "\n",
    "# disctrict data and corps\n",
    "print(f\"District data and Corps shape: {df.shape}\")\n",
    "\n",
    "# Average Yearly Weather Information\n",
    "df_weather = pd.read_csv('dataset/district_with_weather_data.csv')\n",
    "print(f\"Weather information shape: {df_weather.shape}\")\n",
    "\n",
    "#  district elevation and slope\n",
    "df_elevation = pd.read_csv('dataset/elevation and slope dataset 155 districts.csv')\n",
    "print(f\"District elevation and slope shape: {df_elevation.shape}\")\n",
    "\n",
    "# district soil type\n",
    "df_soil = pd.read_csv('dataset/Final_Malaysia_Soil_Dataset__155_Districts_csv.csv')\n",
    "print(f\"District soil type shape: {df_soil.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in here, we see that the disctrict row data is bigger than the climate and elevation data because of multiple crop_type on each district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned - standardized state/district names\n"
     ]
    }
   ],
   "source": [
    "# data cleaning\n",
    "# Clean column names (remove extra spaces)\n",
    "df_weather.columns = df_weather.columns.str.strip()\n",
    "df_elevation.columns = df_elevation.columns.str.strip()\n",
    "df_soil.columns = df_soil.columns.str.strip()\n",
    "# Standardize state and district names (remove extra spaces, standardize case)\n",
    "for dfw in [df_weather, df_elevation, df_soil]:\n",
    "    dfw['state'] = dfw['state'].str.strip().str.title()\n",
    "    dfw['district'] = dfw['district'].str.strip().str.title()\n",
    "\n",
    "print(\"Data cleaned - standardized state/district names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGING DATASETS\n",
      "Starting with crop data: (903, 8)\n",
      "After adding climate data: (903, 20)\n",
      "After adding elevation and slope data: (903, 27)\n",
      "After adding soil data: (903, 32)\n",
      "\n",
      "COMBINED DATASET\n",
      "Final dataset shape: (903, 32)\n",
      "Columns: Index(['state', 'district', 'crop_type', 'crop_rank_1', 'crop_rank_2',\n",
      "       'crop_rank_3', 'crop_rank_4', 'crop_rank_5', 'lat', 'lon',\n",
      "       'uv_index_max', 'temperature_2m_mean', 'cloud_cover_mean',\n",
      "       'relative_humidity_2m_mean', 'sunshine_duration', 'precipitation_sum',\n",
      "       'et0_fao_evapotranspiration', 'temperature_2m_min',\n",
      "       'temperature_2m_max', 'shortwave_radiation_sum', 'elev_mean',\n",
      "       'elev_min', 'elev_max', 'slope_mean', 'slope_min', 'slope_max',\n",
      "       'slope_std', 'Land Use', 'OC (%)', 'OM (%)', 'Soil Depth (cm)',\n",
      "       'Source'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Merging dataset\n",
    "print(\"MERGING DATASETS\")\n",
    "\n",
    "# Step 1: Start with main crop production data (using existing df)\n",
    "combined_df = df.copy()\n",
    "print(f\"Starting with crop data: {combined_df.shape}\")\n",
    "\n",
    "# Step 2: Merge with weather data\n",
    "combined_df = pd.merge(\n",
    "    combined_df, \n",
    "    df_weather, \n",
    "    on=['state', 'district'], \n",
    "    how='left'\n",
    ")\n",
    "print(f\"After adding climate data: {combined_df.shape}\")\n",
    "\n",
    "# step 3: Merge with elevation and slope data\n",
    "combined_df = pd.merge(\n",
    "    combined_df,\n",
    "    df_elevation,\n",
    "    on=['state', 'district'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"After adding elevation and slope data: {combined_df.shape}\")\n",
    "\n",
    "# step 4: Merge with soil data\n",
    "combined_df = pd.merge(\n",
    "    combined_df,\n",
    "    df_soil,\n",
    "    on=['state', 'district'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"After adding soil data: {combined_df.shape}\")\n",
    "print()\n",
    "print(\"COMBINED DATASET\")\n",
    "print(f\"Final dataset shape: {combined_df.shape}\")\n",
    "print(f\"Columns: {combined_df.columns}\")\n",
    "\n",
    "# save the new dataset csv\n",
    "combined_df.to_csv('dataset/district_information.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we can start the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 27\n",
      "Samples: 903\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('dataset/district_information.csv')\n",
    "\n",
    "# Define X and Y\n",
    "Y_column = ['crop_rank_1', 'crop_rank_2', 'crop_rank_3', 'crop_rank_4', 'crop_rank_5']\n",
    "X = df.drop(columns=Y_column)\n",
    "y = df[Y_column]\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVE REDUNDANT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after manual removal: 16\n"
     ]
    }
   ],
   "source": [
    "# remove obviously redundant features\n",
    "redundant = {\n",
    "    'Source',\n",
    "    'temperature_2m_min', 'temperature_2m_max',\n",
    "    'elev_min', 'elev_max',\n",
    "    'slope_min', 'slope_max',\n",
    "    'state', 'district',\n",
    "    'lat', 'lon'\n",
    "}\n",
    "\n",
    "# drop the columns\n",
    "X_reduced = X.drop(redundant, axis=1)\n",
    "print(f\"Features after manual removal: {X_reduced.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Highly correlated features to remove (correlation > 0.85):\n",
      "   - shortwave_radiation_sum\n",
      "   - slope_mean\n",
      "   - OM (%)\n"
     ]
    }
   ],
   "source": [
    "# CORRELATION-BASED REMOVAL\n",
    "def remove_correlated_features(df, threshold=0.85):\n",
    "    \"\"\"Remove highly correlated features\"\"\"\n",
    "    # Only use numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    corr_matrix = df[numeric_cols].corr().abs()\n",
    "    \n",
    "    # Find features to remove\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    to_remove = [column for column in upper_triangle.columns \n",
    "                if any(upper_triangle[column] > threshold)]\n",
    "    \n",
    "    return to_remove, corr_matrix\n",
    "\n",
    "correlated_features, corr_matrix = remove_correlated_features(X_reduced, threshold=0.85)\n",
    "\n",
    "print(f\"ðŸ” Highly correlated features to remove (correlation > 0.85):\")\n",
    "for feature in correlated_features:\n",
    "    print(f\"   - {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after correlation removal: 13\n"
     ]
    }
   ],
   "source": [
    "# remove the highly correlated features \n",
    "X_reduced = X_reduced.drop(columns=correlated_features)\n",
    "print(f\"Features after correlation removal: {X_reduced.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODE CATEGORICAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['crop_type', 'Land Use']\n",
      "Final preprocessed features: 13\n"
     ]
    }
   ],
   "source": [
    "X_processed = X_reduced.copy()\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Handle missing values\n",
    "X_processed = X_processed.fillna(X_processed.median())\n",
    "\n",
    "print(f\"Final preprocessed features: {X_processed.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
